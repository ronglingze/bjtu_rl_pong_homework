import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import os
from collections import namedtuple, deque
from utils.process_obs_tool import ObsProcessTool
# å¯¼å…¥NoisyLinear
from .noisy_layer import NoisyLinear
from .PrioritizedReplayBuffer import PrioritizedReplayBuffer

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ Using device: {device}")
print(f"ğŸ”¥ CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ”¥ CUDA device count: {torch.cuda.device_count()}")
    print(f"ğŸ”¥ Current CUDA device: {torch.cuda.current_device()}")


# å®šä¹‰ç½‘ç»œç»“æ„
class DQN(nn.Module):

    def __init__(self, state_size, action_size, skip_frame=4, horizon=4, clip=False, left=False,std_init = 0.4):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(state_size[0], 32, 8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)

        fc_input_dims = self.calculate_conv_output_dims(state_size)

        # Dueling DQN: å…±äº«ç‰¹å¾æå–å±‚
        self.shared_fc = NoisyLinear(fc_input_dims, 512, std_init = std_init)

        # ä»·å€¼æµ (Value Stream) - è¾“å‡º V(s)
        self.value_stream = NoisyLinear(512, 1,std_init = std_init)

        # ä¼˜åŠ¿æµ (Advantage Stream) - è¾“å‡º A(s,a)
        self.advantage_stream = NoisyLinear(512, action_size,std_init = std_init)

        self.obs_process_tool = ObsProcessTool(skip_frame=skip_frame, horizon=horizon, clip=clip, flip=left)
        self.pre_action = 2

    def calculate_conv_output_dims(self, input_dims):
        state = torch.zeros(1, *input_dims)
        dims = self.conv1(state)
        dims = self.conv2(dims)
        dims = self.conv3(dims)
        return int(np.prod(dims.size()))

    # é‡ç½®ç½‘ç»œä¸­æ‰€æœ‰ Noisy å±‚çš„å™ªå£°
    def reset_noise(self):
        self.shared_fc.reset_noise()
        self.value_stream.reset_noise()
        self.advantage_stream.reset_noise()

    def forward(self, state):
        # å·ç§¯å±‚ç‰¹å¾æå–
        layer = F.relu(self.conv1(state))
        layer = F.relu(self.conv2(layer))
        layer = F.relu(self.conv3(layer))
        layer = layer.view(layer.size()[0], -1)

        # å…±äº«å…¨è¿æ¥å±‚
        shared_features = F.relu(self.shared_fc(layer))

        # åˆ†ç¦»ä¸ºä»·å€¼æµå’Œä¼˜åŠ¿æµ
        value = self.value_stream(shared_features)  # V(s) - [batch_size, 1]
        advantage = self.advantage_stream(shared_features)  # A(s,a) - [batch_size, action_size]

        # Dueling DQN: Q(s,a) = V(s) + [A(s,a) - mean(A(s,a))]
        # è¿™æ ·å¯ä»¥è§£å†³å¯è¯†åˆ«æ€§é—®é¢˜
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))

        return q_values

    def act(self, obs):
        code, state = self.obs_process_tool.process(obs)
        if code == -1:
            return self.pre_action
        else:
            state = torch.from_numpy(np.float32(state)).unsqueeze(0).to(device)
            # actè°ƒç”¨å‰ï¼ŒAgenté€šå¸¸å·²ç»é‡ç½®è¿‡å™ªå£°
            with torch.no_grad():
                q_val = self.forward(state)
            act = q_val.max(1)[1].item()
            self.pre_action = act
            return act


# å®šä¹‰ä»£ç†ç±»
class DQNAgent:

    def __init__(self, state_size, action_size, batch_size=64, gamma=0.995, lr=5e-5, memory_size=20000, skip_frame=4, horizon=4, clip=False, left=False,std_init=0.2):
        self.state_size = state_size
        self.action_size = action_size
        self.batch_size = batch_size
        self.gamma = gamma
        self.lr = lr

        self.dqn_net = DQN(self.state_size, self.action_size, skip_frame=skip_frame, horizon=horizon, clip=clip, left=left,std_init=std_init).to(device)
        self.target_net = DQN(self.state_size, self.action_size, skip_frame=skip_frame, horizon=horizon, clip=clip, left=left,std_init=std_init).to(device)
        self.optimizer = optim.Adam(self.dqn_net.parameters(), lr=self.lr)

        # ä½¿ç”¨PrioritizedReplayBufferæ›¿ä»£åŸæ¥çš„deque
        self.memory = PrioritizedReplayBuffer(capacity=memory_size)

        # ç§»é™¤äº† epsilon ç›¸å…³å‚æ•°ï¼Œå› ä¸ºç”± NoisyNet å…¨æƒæ¥ç®¡æ¢ç´¢

    def select_action(self, state, eps=None):
        # 1. é‡ç½®å™ªå£°ï¼Œç¡®ä¿æ¢ç´¢æ€§
        self.dqn_net.reset_noise()

        # 2. ç›´æ¥æ ¹æ®ç½‘ç»œè¾“å‡ºé€‰æ‹©åŠ¨ä½œ (ä¸å†ä½¿ç”¨ epsilon-greedy)
        act = self.dqn_net.act(state)
        return act

    def memory_push(self, state, action, next_state, reward, done):
        # å¯¹äºæ–°ç»éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå¤§çš„åˆå§‹ä¼˜å…ˆçº§ä»¥ç¡®ä¿å®ƒä»¬è‡³å°‘è¢«å­¦ä¹ ä¸€æ¬¡
        # TD-errorå°†åœ¨åç»­æ›´æ–°ä¸­è®¡ç®—å¹¶æ›´æ–°
        max_priority = 1.0
        self.memory.push(max_priority, (state, action, next_state, reward, done))

    def update(self, step, return_values=False):
        if len(self.memory) < self.batch_size:
            if return_values:
                return [], []
            return

        self.dqn_net.train()
        self.update_target_net(step)

        self.optimizer.zero_grad()

        # é‡ç½®å™ªå£°
        self.dqn_net.reset_noise()
        self.target_net.reset_noise()
        
        # ä»ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·
        states, actions, next_states, rewards, dones, indices, is_weights = self.memory.sample(self.batch_size)

        states = torch.from_numpy(np.float32(states)).to(device)
        actions = torch.from_numpy(actions).to(device)
        next_states = torch.from_numpy(np.float32(next_states)).to(device)
        rewards = torch.from_numpy(rewards).to(device)
        dones = torch.from_numpy(dones).to(device)
        is_weights = torch.from_numpy(is_weights).to(device)

        q_vals = self.dqn_net(states)
        if actions.dtype != torch.int64:
            actions = actions.long()
        q_val = q_vals.gather(1, actions.unsqueeze(-1)).squeeze(-1)

        # Double DQN ç›®æ ‡
        with torch.no_grad():
            next_online_q = self.dqn_net(next_states)
            next_actions = next_online_q.max(1)[1]
            next_target_q = self.target_net(next_states)
            nxt_q_val = next_target_q.gather(1, next_actions.unsqueeze(-1)).squeeze(-1)

        exp_q_val = rewards + self.gamma * nxt_q_val * (1 - dones)

        # TD-error
        td_errors = (q_val - exp_q_val).detach().cpu().numpy()

        per_sample_loss = F.smooth_l1_loss(q_val, exp_q_val, reduction='none')
        is_weights = is_weights.to(per_sample_loss.device).float()
        loss = (is_weights * per_sample_loss).mean()

        loss.backward()
        self.optimizer.step()

        # æ›´æ–°ç»éªŒä¼˜å…ˆçº§
        self.memory.update_priorities(indices, td_errors)

        # Q å€¼å‡å€¼
        q_mean_val = q_val.mean().item()
        loss_val = loss.item()

        if return_values:
            return loss_val, q_mean_val


    def save_model(self, episode, path):
        torch.save(self.dqn_net.state_dict(), os.path.join(path, 'eval_checkpoint_{}.pth'.format(episode)))
        torch.save(self.target_net.state_dict(), os.path.join(path, 'target_checkpoint_{}.pth'.format(episode)))

    def load_model(self, episode, path):
        self.dqn_net.load_state_dict(torch.load(os.path.join(path, 'eval_checkpoint_{}.pth'.format(episode))))
        self.target_net.load_state_dict(torch.load(os.path.join(path, 'target_checkpoint_{}.pth'.format(episode))))

    def update_target_net(self, step):
        if step % 1000 == 0:
            self.target_net.load_state_dict(self.dqn_net.state_dict())

    def update_epsilon(self, step):
        # NoisyNetä¸éœ€è¦epsilonï¼Œè¿”å›0.0
        return 0.0

    def reset(self):
        self.dqn_net.obs_process_tool.reset()
        self.target_net.obs_process_tool.reset()
        # é‡ç½®ç¯å¢ƒæ—¶ä¹Ÿé‡ç½®å™ªå£°
        self.dqn_net.reset_noise()
        self.target_net.reset_noise()
